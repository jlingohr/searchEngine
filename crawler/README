TODO
-----
- Stop safely for problems
- Check if a file already exists and dont append to if it already exists

***Crawler Design Spec***
-------------------------

(1) *Input*
 
Command Input

./crawler [SEED URL] [TARGET DIRECTORY WHERE TO PUT THE DATA] [MAX CRAWLING DEPTH]

[SEED URL]
Requirement: The URL must be valid.
Usage: The crawler needs to inform the user if URL is not found

[TARGET DIRECTORY] ./data/
Requirement: The directory must exist and be already created by the user.
Usage: The crawler needs to inform the user if the directory can not be found

[MAX CRAWLING DEPTH] 2
Requirement: The crawl depth cannot exceed 4
Usage: The crawler needs to inform the user the user exceeds the maximum depth


------------------------------------------------------------------------
(2) *Output*

 For each webpage crawled the crawler program will create a file in the
[TARGET DIRECTORY]. The name of the file will start at 1 for the [SEED URL]
and be incremented for each subsequent HTML webpage crawled.

Each of these files will include the URL associated with a webpage as the 
first line of the file, the depth at which that webpage was reached on the 
second line, and the HTML for the webpage will copied into the file starting 
on the third line.


------------------------------------------------------------------------
(3) *Data Flow*

The seed web page contents (HTML) are read and stored in a new webPage data structure. 

The HTML is then parsed for URLs. Each URL found on the page is checked to see whether
it is already in the hashTable of URL's we've seen AND whether visiting that URL would 
exceed our depth limit. If not, the URL is saved on a list of URL's we still need to 
visit.  This list is the URLList.

After being scanned for URL's, the webpage is stored in a local file having filenames
1, 2, 3, and so on. The webpage is added to the hashTable to indicate we've visited it.

Once a webpage is copied to a file and all its new URL's saved on the URLList, 
a URL is taken off the list, fetched from the Internet, stored in a file, and 
parsed for URLs, which are added to the URLList if they're not already there. 

We continue ths process as long as there are URL's on the URLList.


------------------------------------------------------------------------
(4) *Data Structures*
---------------------

    maxDepth - maximum depth to crawl.

    page - pointer to the current page (the string that holds the current webpage)

    URLList - holds a list of URLs from the page by name only

    WebPage structure holds the URL name, a pointer to the HTML source, the length 
    of the HTML source, and the depth of the webpage.

    URLToBeVisited - next URL to be visited.

Clearly, we need a data structure that will allow us to store URLs that are
returned IFF they are UNIQUE. At this stage the definition of a double linked
list would be sufficient to manage the storing and searching (to see if a new URL
is already on the list. If so it is not added). We define a double link list
with a start and end.

    ListNode has a URL, previous pointer, next pointer

For now it is fine just to understand that that this is a list of some
data structure that holds information about the URL (such as WebPage information).
In the Implementation Spec we will flush out the detailed C structures for all
of the above. But for now abstraction is fine. Details will come in the
Implementation Spec.

We also need some way to track the URL's we've processed.  We keep hearing about
a cool data structure called a HashTable, so maybe we'll use that.  