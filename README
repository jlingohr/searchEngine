Search Engine

--------
CRAWLER

  Usage: ./crawler [URL] [Directory] [Depth]

  Note: Crawler limited to crawling domain specified by URL_PREFIX for security purposes
  and crawls to a depth of at most 4

  TODO
  -----
  - Stop safely for problems
  - Check if a file already exists and dont append to if it already exists
  - Valgrind errors gone, but some memory leaks remain; have to check free conditions in crawPage and freeWebPage, list_dequeue


---------
INDEXER

  Usage: ./index [Crawl Directory] [indexed.dat]

  Assumptions: Indexer assumes that webpages crawled will be placed in in [Crawl Directory]
  by the crawler

  TODOS:
  ----------
  - Fix any memory leaks - seg fault only wen running valgrind

  -Tidy up
  -Seperate modules/Libraries

--------
Query

  Usage: ./query [indexed.dat] [Crawl Dir]

  Assumptions: Query assumes that [indexed.dat] is a valid file and [Crawl Dir] is a valid directory

  TODO
  - Test that intersect works
  - Start testing on multuple pages
  - exit nicely
  - memory leaks and dangling pointers
  - Move intersect into list.c and make generic
  - Add kill signal

__________
TODO (Program)
- System testing
- complete unit testing
- Fix all memory leaks 